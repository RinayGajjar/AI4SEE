{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRugHKGZ4gE8w1f8mdy9DG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RinayGajjar/AI4SEE/blob/main/AI4SEE_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4JzDQ_Is9Ay",
        "outputId": "80d764ee-706b-44cb-d32b-a254f148e835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating chunk 1/10...\n",
            "Generating chunk 2/10...\n",
            "Generating chunk 3/10...\n",
            "Generating chunk 4/10...\n",
            "Generating chunk 5/10...\n",
            "Generating chunk 6/10...\n",
            "Generating chunk 7/10...\n",
            "Generating chunk 8/10...\n",
            "Generating chunk 9/10...\n",
            "Generating chunk 10/10...\n",
            "1GB dataset generated and saved to synthetic_hardware_monitor_1gb.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Settings\n",
        "target_size_gb = 1  # Target size in GB\n",
        "row_size_bytes = 100  # Estimated average size of one row in bytes\n",
        "num_rows = (target_size_gb * 1024**3) // row_size_bytes  # Total number of rows needed\n",
        "sampling_rate_hz = 10   # 10 samples per second\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "# Generate synthetic data in chunks to avoid memory overload\n",
        "chunk_size = 1_000_000  # Number of rows per chunk\n",
        "num_chunks = num_rows // chunk_size\n",
        "\n",
        "# Function to generate a chunk of data\n",
        "def generate_chunk(start_index, chunk_size):\n",
        "    timestamps = []\n",
        "    cpu_temperatures = []\n",
        "    cpu_usages = []\n",
        "    cpu_loads = []\n",
        "    memory_usages = []\n",
        "    battery_levels = []\n",
        "    cpu_powers = []\n",
        "\n",
        "    for i in range(chunk_size):\n",
        "        current_time = start_time + datetime.timedelta(seconds=(start_index + i) / sampling_rate_hz)\n",
        "        timestamps.append(current_time)\n",
        "\n",
        "        # Generate random normal data\n",
        "        cpu_temp = max(20, min(85, random.gauss(50, 10)))  # CPU temperature (20-85Â°C)\n",
        "        cpu_usage = max(0, min(100, random.gauss(50, 20)))  # CPU usage (%)\n",
        "        cpu_load = random.uniform(0, 2.5)  # 1-minute load average\n",
        "        memory_usage = random.uniform(30, 90)  # Memory usage (%)\n",
        "        battery_level = random.uniform(20, 100)  # Battery level (%)\n",
        "        cpu_power = random.uniform(10, 50)  # CPU power consumption (W)\n",
        "\n",
        "        # Introduce anomalies randomly\n",
        "        if random.random() < 0.1:\n",
        "            cpu_usage = random.uniform(90, 100)  # High CPU usage\n",
        "        if random.random() < 0.05:\n",
        "            cpu_temp = random.uniform(85, 105)  # High CPU temperature\n",
        "        if random.random() < 0.05:\n",
        "            memory_usage = random.uniform(90, 100)  # High memory usage\n",
        "        if random.random() < 0.02:\n",
        "            battery_level = random.uniform(0, 10)  # Low battery level\n",
        "        if random.random() < 0.03:\n",
        "            cpu_power = random.uniform(50, 100)  # High CPU power\n",
        "\n",
        "        # Append to lists\n",
        "        cpu_temperatures.append(cpu_temp)\n",
        "        cpu_usages.append(cpu_usage)\n",
        "        cpu_loads.append(cpu_load)\n",
        "        memory_usages.append(memory_usage)\n",
        "        battery_levels.append(battery_level)\n",
        "        cpu_powers.append(cpu_power)\n",
        "\n",
        "    # Create a DataFrame for the chunk\n",
        "    chunk_data = {\n",
        "        'timestamp': timestamps,\n",
        "        'cpu_temperature': cpu_temperatures,\n",
        "        'cpu_usage': cpu_usages,\n",
        "        'cpu_load': cpu_loads,\n",
        "        'memory_usage': memory_usages,\n",
        "        'battery_level': battery_levels,\n",
        "        'cpu_power': cpu_powers,\n",
        "    }\n",
        "    return pd.DataFrame(chunk_data)\n",
        "\n",
        "# Generate and save the dataset in chunks\n",
        "output_file = 'synthetic_hardware_monitor_1gb.csv'\n",
        "with open(output_file, 'w') as f:\n",
        "    for chunk_index in range(num_chunks):\n",
        "        print(f\"Generating chunk {chunk_index + 1}/{num_chunks}...\")\n",
        "        start_row = chunk_index * chunk_size\n",
        "        chunk_df = generate_chunk(start_row, chunk_size)\n",
        "\n",
        "        # Save chunk to file\n",
        "        chunk_df.to_csv(f, index=False, header=(chunk_index == 0))  # Write header only for the first chunk\n",
        "\n",
        "print(f\"1GB dataset generated and saved to {output_file}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bU8IlK64tF3d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}